{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tf-keras in c:\\python311\\lib\\site-packages (2.16.0)\n",
      "Requirement already satisfied: tensorflow<2.17,>=2.16 in c:\\users\\winto\\appdata\\roaming\\python\\python311\\site-packages (from tf-keras) (2.16.1)\n",
      "Requirement already satisfied: tensorflow-intel==2.16.1 in c:\\users\\winto\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow<2.17,>=2.16->tf-keras) (2.16.1)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\python311\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\python311\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in c:\\python311\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\python311\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (0.5.4)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\python311\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in c:\\python311\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (3.10.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\python311\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes~=0.3.1 in c:\\python311\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (0.3.2)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\python311\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (3.3.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\winto\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (24.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\python311\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (4.25.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\winto\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (2.31.0)\n",
      "Requirement already satisfied: setuptools in c:\\python311\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (65.5.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\winto\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\python311\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\python311\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (4.11.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\python311\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (1.16.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\python311\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (1.62.1)\n",
      "Requirement already satisfied: tensorboard<2.17,>=2.16 in c:\\python311\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (2.16.2)\n",
      "Requirement already satisfied: keras>=3.0.0 in c:\\users\\winto\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (3.1.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\python311\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (0.31.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in c:\\python311\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (1.26.4)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\python311\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (0.43.0)\n",
      "Requirement already satisfied: rich in c:\\users\\winto\\appdata\\roaming\\python\\python311\\site-packages (from keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (13.7.1)\n",
      "Requirement already satisfied: namex in c:\\python311\\lib\\site-packages (from keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (0.0.7)\n",
      "Requirement already satisfied: optree in c:\\python311\\lib\\site-packages (from keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (0.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\python311\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\python311\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\python311\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\python311\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (2024.2.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\python311\\lib\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (3.6)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\python311\\lib\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\python311\\lib\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (3.0.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\python311\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (2.1.5)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\python311\\lib\\site-packages (from rich->keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\winto\\appdata\\roaming\\python\\python311\\site-packages (from rich->keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (2.17.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\python311\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (0.1.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install tf-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\WinTo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "# Import all the required libraries\n",
    "# Use Kaggle's pre-tuned notebooks to get the optimal versions of all the dependencies\n",
    "\n",
    "import nltk\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "nltk.download('stopwords')\n",
    "from string import punctuation\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Python311\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15fec4600d9646d69d47d380eff01467",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.63G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a2581447c41438c8316adcfb01b7948",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62eed7355226497eb73362fe5a320934",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ad0bba1f3b349c28314704f82cbea6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de95f3b22afd4fa0bff9e34ae6406e75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import all the required libraries\n",
    "from transformers import pipeline\n",
    "classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframe for final sentiment classification result\n",
    "def createDataFrame(labels, confidence, tweet):\n",
    "    labels = pd.DataFrame({'Labels': labels})\n",
    "    confidence = pd.DataFrame({'Confidence Scores': confidence})\n",
    "    column_values = ['Labels', 'Confidence']\n",
    "    sentiment_scores = pd.concat([labels,confidence], ignore_index=False, axis=1)\n",
    "    print(\"\\n--------------------------------------------------------------------------------------\")\n",
    "    print(f\"\\n Entered input sentence: {tweet}\")\n",
    "    print(\"\\n Sentiment of the tweet (Probability Distribution): \")\n",
    "    print(sentiment_scores.to_string(index=False))\n",
    "    #print(\"--------------------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_bart(tweet):\n",
    "    labels = []\n",
    "    confidence = []\n",
    "    \n",
    "    # Possible Sentiment Categories\n",
    "    candidate_labels = [\"happy\", \"sad\", \"warn\", \"angry\", \"sorrow\", \"alert\", \"neutral\"]\n",
    "    #candidate_labels = [\"OYC\", \"DTC\", \"NCF\", \"KNY\", \"DCF\"]\n",
    "    \n",
    "    # Send the labels and tweets to the classifier pipeline\n",
    "    result = classifier(tweet, candidate_labels)\n",
    "    \n",
    "    # Extract the labels from results dictionary\n",
    "    labels.append(result[\"labels\"])\n",
    "    labels = [item for sublist in labels for item in sublist] # Flatten the list of lists into list\n",
    "    \n",
    "    # Extract the labels from results dictionary\n",
    "    confidence.append(result[\"scores\"])\n",
    "    confidence = [(str(float(item)*100))[:6]+\" %\" for sublist in confidence for item in sublist] # Flatten the list of lists into list\n",
    "\n",
    "    createDataFrame(labels,confidence, tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Sentiment Analysis of COVID-19 Tweets with BART\n",
      "\n",
      "------Available Options------\n",
      "1. Inference on Sample Tweets\n",
      "2. Enter Custom Tweets/Sentences\n",
      "3. Exit\n",
      "\n",
      "Please select an option from the above:\n",
      "\n",
      "--------------------------------------------------------------------------------------\n",
      "\n",
      " Entered input sentence: Many lost their jobs because of covid and it is highly dangerous\n",
      "\n",
      " Sentiment of the tweet (Probability Distribution): \n",
      " Labels Confidence Scores\n",
      "   warn          51.956 %\n",
      "  alert          26.361 %\n",
      "    sad          12.751 %\n",
      " sorrow          4.9466 %\n",
      "  angry          3.1605 %\n",
      "neutral          0.5962 %\n",
      "  happy          0.2266 %\n",
      "\n",
      "--------------------------------------------------------------------------------------\n",
      "\n",
      " Entered input sentence: I am happy that my family members are safe in this tough times\n",
      "\n",
      " Sentiment of the tweet (Probability Distribution): \n",
      " Labels Confidence Scores\n",
      "  happy          76.314 %\n",
      "  alert          17.165 %\n",
      "   warn          3.8741 %\n",
      "neutral          1.7418 %\n",
      " sorrow          0.3840 %\n",
      "    sad          0.3506 %\n",
      "  angry          0.1697 %\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nwhile(True):\\n    choice = int(input())\\n\\n    if choice == 1:\\n        sample_1 = \\'Many lost their jobs because of covid and it is highly dangerous\\'\\n        sentiment_bart(sample_1)\\n    \\n        sample_2 = \\'I am happy that my family members are safe in this tough times\\'\\n        sentiment_bart(sample_2)\\n    \\n    elif choice == 2:\\n        print(\"\\nPlease enter a sentence/tweet:\")\\n        user_input = input()\\n        sentiment_bart(user_input)\\n    \\n    elif choice == 3:\\n        print(\"\\nExiting...\")\\n        break\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Driver program\n",
    "print(\"Neural Sentiment Analysis of COVID-19 Tweets with BART\")\n",
    "print(\"\\n------Available Options------\")\n",
    "print(\"1. Inference on Sample Tweets\")\n",
    "print(\"2. Enter Custom Tweets/Sentences\")\n",
    "print(\"3. Exit\")\n",
    "print(\"\\nPlease select an option from the above:\")\n",
    "\n",
    "\n",
    "sample_1 = 'Many lost their jobs because of covid and it is highly dangerous'\n",
    "sentiment_bart(sample_1)\n",
    "    \n",
    "sample_2 = 'I am happy that my family members are safe in this tough times'\n",
    "sentiment_bart(sample_2)\n",
    "\n",
    "\"\"\"\n",
    "while(True):\n",
    "    choice = int(input())\n",
    "\n",
    "    if choice == 1:\n",
    "        sample_1 = 'Many lost their jobs because of covid and it is highly dangerous'\n",
    "        sentiment_bart(sample_1)\n",
    "    \n",
    "        sample_2 = 'I am happy that my family members are safe in this tough times'\n",
    "        sentiment_bart(sample_2)\n",
    "    \n",
    "    elif choice == 2:\n",
    "        print(\"\\nPlease enter a sentence/tweet:\")\n",
    "        user_input = input()\n",
    "        sentiment_bart(user_input)\n",
    "    \n",
    "    elif choice == 3:\n",
    "        print(\"\\nExiting...\")\n",
    "        break\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the transfer learning tweet dataset\n",
    "sentiment_df = pd.read_csv('twitterdata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parallely Processing using CUDA\n"
     ]
    }
   ],
   "source": [
    "# Checking if NVIDIA Graphics Card and CUDA is available\n",
    "gpu_available = torch.cuda.is_available\n",
    "\n",
    "if gpu_available:\n",
    "    print('Parallely Processing using CUDA')\n",
    "else:\n",
    "    print('No CUDA Detected')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-process the text and perform Stemming, Lemmatization and Stop-word removal\n",
    "def text_preprocessing(text):\n",
    "    remove_punctuation = [ch for ch in text if ch not in punctuation]\n",
    "    remove_punctuation = \"\".join(remove_punctuation).split()\n",
    "    filtered_text = [word.lower() for word in remove_punctuation if word.lower() not in stopwords.words('english')]\n",
    "    return filtered_text\n",
    "\n",
    "\n",
    "# Pad blank topken to keep the length of tweets consistent - mandatory to normalize and train the model\n",
    "def pad_features(reviews_int, seq_length):\n",
    "    features = np.zeros((len(reviews_int), seq_length), dtype=int)\n",
    "    for i, row in enumerate(reviews_int):\n",
    "        if len(row)!=0:\n",
    "            features[i, -len(row):] = np.array(row)[:seq_length]\n",
    "    return features\n",
    "\n",
    "# Convert the sentences into stream of tokens\n",
    "def tokenize(tweet):\n",
    "    test_ints = []\n",
    "    test_ints.append([vocab_to_int[word] for word in tweet])\n",
    "    return test_ints\n",
    "\n",
    "# Predict the sentiment of the tweet - performs binary classification using the model inference\n",
    "def sentiment(net, test_tweet, seq_length=50):\n",
    "    print(\"\\n--------------------------------------------------------------------------------------\")\n",
    "    print(f\"\\n Original input sentence: {test_tweet}\")\n",
    "    test_tweet = text_preprocessing(test_tweet)\n",
    "    tokenized_tweet = tokenize(test_tweet)\n",
    "    \n",
    "    print(f\"\\n Pre-processed input sentence: {test_tweet}\")\n",
    "    #print(f\"\\nSentence converted into tokens:\\n{tokenized_tweet}\")\n",
    "    \n",
    "    padded_tweet = pad_features(tokenized_tweet, 50)\n",
    "    feature_tensor = torch.from_numpy(padded_tweet)\n",
    "    batch_size = feature_tensor.size(0)\n",
    "    \n",
    "    if gpu_available:\n",
    "        feature_tensor = feature_tensor.cuda()\n",
    "    \n",
    "    h = net.init_hidden(batch_size)\n",
    "    output, h = net(feature_tensor, h)\n",
    "    \n",
    "    predicted_sentiment = torch.round(output.squeeze())\n",
    "    \n",
    "    if predicted_sentiment == 1:\n",
    "        print(\"\\n Sentiment: Positive\")\n",
    "        \n",
    "    else:\n",
    "        print(\"\\n Sentiment: Negative\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code block to invoke Pre-processing, Padding and Tokenization operations on the tweet\n",
    "\n",
    "sentiment_df.loc[:, 'text'] = sentiment_df['text'].apply(text_preprocessing)\n",
    "\n",
    "reviews_split = []\n",
    "for i, j in sentiment_df.iterrows():\n",
    "    reviews_split.append(j['text'])\n",
    "\n",
    "words = []\n",
    "for review in reviews_split:\n",
    "    for word in review:\n",
    "        words.append(word)\n",
    "\n",
    "counts = Counter(words)\n",
    "vocab = sorted(counts, key=counts.get, reverse=True)\n",
    "vocab_to_int = {word:ii for ii, word in enumerate(vocab, 1)}\n",
    "\n",
    "encoded_reviews = []\n",
    "for review in reviews_split:\n",
    "    encoded_reviews.append([vocab_to_int[word] for word in review])\n",
    "\n",
    "labels_to_int = []\n",
    "for i, j in sentiment_df.iterrows():\n",
    "    if j['sentiment']=='joy':\n",
    "        labels_to_int.append(1)\n",
    "    else:\n",
    "        labels_to_int.append(0)\n",
    "\n",
    "reviews_len = Counter([len(x) for x in encoded_reviews])\n",
    "non_zero_idx = [ii for ii, review in enumerate(encoded_reviews) if len(encoded_reviews)!=0]\n",
    "encoded_reviews = [encoded_reviews[ii] for ii in non_zero_idx]\n",
    "encoded_labels = np.array([labels_to_int[ii] for ii in non_zero_idx])\n",
    "\n",
    "seq_length = 50\n",
    "padded_features= pad_features(encoded_reviews, seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into Train (80%), Validation (10%) & Test (10%)\n",
    "batch_size = 1\n",
    "split_frac = 0.8\n",
    "split_idx = int(len(padded_features)*split_frac)\n",
    "\n",
    "training_x, remaining_x = padded_features[:split_idx], padded_features[split_idx:]\n",
    "training_y, remaining_y = encoded_labels[:split_idx], encoded_labels[split_idx:]\n",
    "\n",
    "test_idx = int(len(remaining_x)*0.5)\n",
    "val_x, test_x = remaining_x[:test_idx], remaining_x[test_idx:]\n",
    "val_y, test_y = remaining_y[:test_idx], remaining_y[test_idx:]\n",
    "\n",
    "# Transform the data into a Tensor datastructure\n",
    "train_data = TensorDataset(torch.from_numpy(training_x), torch.from_numpy(training_y))\n",
    "test_data = TensorDataset(torch.from_numpy(test_x), torch.from_numpy(test_y))\n",
    "valid_data = TensorDataset(torch.from_numpy(val_x), torch.from_numpy(val_y))\n",
    "\n",
    "# Prepare the dataloader for Train, Test and Validation\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size)\n",
    "valid_loader = DataLoader(valid_data, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding Dimension of Tokens\n",
    "embedding_dim = 400\n",
    "\n",
    "# Embedding Dimension of Hidden Layers\n",
    "hidden_dim = 256\n",
    "\n",
    "# Output of the model is binary (either Positive or Negative)\n",
    "output_size = 1\n",
    "\n",
    "# Number of hidden LSTM cells\n",
    "n_layers = 2\n",
    "vocab_size = len(vocab_to_int)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Structure of the Neural Network\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, drop_prob=0.2):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embedding_layer = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=drop_prob, batch_first=True)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "        self.sig = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        batch_size = x.size(0)\n",
    "        x = x.long()\n",
    "        embeds = self.embedding_layer(x)\n",
    "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
    "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
    "        out = self.dropout(lstm_out)\n",
    "        out = self.fc(out)\n",
    "        sig_out = self.sig(out)\n",
    "        sig_out = sig_out.view(batch_size, -1)\n",
    "        sig_out = sig_out[:, -1]\n",
    "        return sig_out, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        weights = next(self.parameters()).data\n",
    "        if gpu_available:\n",
    "            hidden = (weights.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),weights.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n",
    "        else:\n",
    "            hidden = (weights.new(self.n_layers, batch_size, self.hidden_dim).zero_(),weights.new(self.n_layers, batch_size, self.hidden_dim).zero())\n",
    "        return hidden\n",
    "\n",
    "net = LSTM(vocab_size, output_size, embedding_dim, hidden_dim, n_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters required for training of the network\n",
    "\n",
    "# Learning Rate\n",
    "lr = 0.001\n",
    "\n",
    "# Loss Function - Binary Cross Entropy\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Gradient Descent based Optimizer - ADAM (Adaptive LR)\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "\n",
    "# Number of epochs to train the model\n",
    "epochs = 8\n",
    "count = 0\n",
    "\n",
    "# Step size\n",
    "print_every = 200\n",
    "clip = 5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\WinTo\\AppData\\Local\\Temp\\ipykernel_9604\\1713570961.py:22: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(net.parameters(), clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/8..... Step: 400..... Train Loss: 0.002401...... Validation Loss: 0.002588\n",
      "Epoch: 1/8..... Step: 600..... Train Loss: 0.073231...... Validation Loss: 0.085619\n",
      "Epoch: 1/8..... Step: 800..... Train Loss: 0.129901...... Validation Loss: 0.150511\n",
      "Epoch: 1/8..... Step: 1000..... Train Loss: 1.220293...... Validation Loss: 0.199739\n",
      "Epoch: 1/8..... Step: 1200..... Train Loss: 0.020662...... Validation Loss: 0.041623\n",
      "Epoch: 1/8..... Step: 1400..... Train Loss: 0.994674...... Validation Loss: 0.357039\n",
      "Epoch: 1/8..... Step: 1600..... Train Loss: 0.046433...... Validation Loss: 0.081051\n",
      "Epoch: 1/8..... Step: 1800..... Train Loss: 0.140046...... Validation Loss: 0.099699\n",
      "Epoch: 1/8..... Step: 2000..... Train Loss: 0.036088...... Validation Loss: 0.764285\n",
      "Epoch: 1/8..... Step: 2200..... Train Loss: 0.332260...... Validation Loss: 0.075611\n",
      "Epoch: 1/8..... Step: 2400..... Train Loss: 0.056512...... Validation Loss: 0.115505\n",
      "Epoch: 1/8..... Step: 2600..... Train Loss: 0.050839...... Validation Loss: 0.160248\n",
      "Epoch: 2/8..... Step: 2800..... Train Loss: 0.012354...... Validation Loss: 0.173505\n",
      "Epoch: 2/8..... Step: 3000..... Train Loss: 0.058442...... Validation Loss: 0.074833\n",
      "Epoch: 2/8..... Step: 3200..... Train Loss: 0.011777...... Validation Loss: 0.084628\n",
      "Epoch: 2/8..... Step: 3400..... Train Loss: 0.003945...... Validation Loss: 0.036916\n",
      "Epoch: 2/8..... Step: 3600..... Train Loss: 0.025756...... Validation Loss: 0.259936\n",
      "Epoch: 2/8..... Step: 3800..... Train Loss: 2.910313...... Validation Loss: 0.046980\n",
      "Epoch: 2/8..... Step: 4000..... Train Loss: 0.007487...... Validation Loss: 0.102639\n",
      "Epoch: 2/8..... Step: 4200..... Train Loss: 0.022462...... Validation Loss: 0.016999\n",
      "Epoch: 2/8..... Step: 4400..... Train Loss: 0.006753...... Validation Loss: 0.218187\n",
      "Epoch: 2/8..... Step: 4600..... Train Loss: 0.006045...... Validation Loss: 0.010930\n",
      "Epoch: 2/8..... Step: 4800..... Train Loss: 2.154489...... Validation Loss: 0.013047\n",
      "Epoch: 2/8..... Step: 5000..... Train Loss: 0.010981...... Validation Loss: 0.006585\n",
      "Epoch: 3/8..... Step: 5200..... Train Loss: 0.001378...... Validation Loss: 0.004540\n",
      "Epoch: 3/8..... Step: 5400..... Train Loss: 0.000345...... Validation Loss: 0.001980\n",
      "Epoch: 3/8..... Step: 5600..... Train Loss: 0.000739...... Validation Loss: 0.001379\n",
      "Epoch: 3/8..... Step: 5800..... Train Loss: 0.003234...... Validation Loss: 0.000458\n",
      "Epoch: 3/8..... Step: 6000..... Train Loss: 0.000074...... Validation Loss: 0.000353\n",
      "Epoch: 3/8..... Step: 6200..... Train Loss: 0.010853...... Validation Loss: 0.000986\n",
      "Epoch: 3/8..... Step: 6400..... Train Loss: 0.004541...... Validation Loss: 0.001500\n",
      "Epoch: 3/8..... Step: 6600..... Train Loss: 0.001540...... Validation Loss: 0.002215\n",
      "Epoch: 3/8..... Step: 6800..... Train Loss: 0.002875...... Validation Loss: 0.007418\n",
      "Epoch: 3/8..... Step: 7000..... Train Loss: 0.000887...... Validation Loss: 0.008120\n",
      "Epoch: 3/8..... Step: 7200..... Train Loss: 0.002099...... Validation Loss: 0.001864\n",
      "Epoch: 3/8..... Step: 7400..... Train Loss: 0.001376...... Validation Loss: 0.001875\n",
      "Epoch: 3/8..... Step: 7600..... Train Loss: 0.007567...... Validation Loss: 0.002016\n",
      "Epoch: 4/8..... Step: 7800..... Train Loss: 0.000143...... Validation Loss: 0.000898\n",
      "Epoch: 4/8..... Step: 8000..... Train Loss: 0.000139...... Validation Loss: 0.001002\n",
      "Epoch: 4/8..... Step: 8200..... Train Loss: 0.000122...... Validation Loss: 0.000341\n",
      "Epoch: 4/8..... Step: 8400..... Train Loss: 0.016713...... Validation Loss: 0.000275\n",
      "Epoch: 4/8..... Step: 8600..... Train Loss: 0.003515...... Validation Loss: 0.000292\n",
      "Epoch: 4/8..... Step: 8800..... Train Loss: 0.000201...... Validation Loss: 0.001475\n",
      "Epoch: 4/8..... Step: 9000..... Train Loss: 0.000615...... Validation Loss: 0.000588\n",
      "Epoch: 4/8..... Step: 9200..... Train Loss: 0.000466...... Validation Loss: 0.000848\n",
      "Epoch: 4/8..... Step: 9400..... Train Loss: 0.000758...... Validation Loss: 0.000515\n",
      "Epoch: 4/8..... Step: 9600..... Train Loss: 0.000098...... Validation Loss: 0.000623\n",
      "Epoch: 4/8..... Step: 9800..... Train Loss: 0.000081...... Validation Loss: 0.000210\n",
      "Epoch: 4/8..... Step: 10000..... Train Loss: 0.000115...... Validation Loss: 0.000556\n",
      "Epoch: 5/8..... Step: 10200..... Train Loss: 0.000072...... Validation Loss: 0.000100\n",
      "Epoch: 5/8..... Step: 10400..... Train Loss: 0.000026...... Validation Loss: 0.000073\n",
      "Epoch: 5/8..... Step: 10600..... Train Loss: 0.000271...... Validation Loss: 0.000395\n",
      "Epoch: 5/8..... Step: 10800..... Train Loss: 0.000058...... Validation Loss: 0.000076\n",
      "Epoch: 5/8..... Step: 11000..... Train Loss: 0.000010...... Validation Loss: 0.000063\n",
      "Epoch: 5/8..... Step: 11200..... Train Loss: 0.000046...... Validation Loss: 0.000968\n",
      "Epoch: 5/8..... Step: 11400..... Train Loss: 0.000123...... Validation Loss: 0.000191\n",
      "Epoch: 5/8..... Step: 11600..... Train Loss: 0.000070...... Validation Loss: 0.000042\n",
      "Epoch: 5/8..... Step: 11800..... Train Loss: 0.000034...... Validation Loss: 0.000086\n",
      "Epoch: 5/8..... Step: 12000..... Train Loss: 0.000005...... Validation Loss: 0.000064\n",
      "Epoch: 5/8..... Step: 12200..... Train Loss: 0.000013...... Validation Loss: 0.000041\n",
      "Epoch: 5/8..... Step: 12400..... Train Loss: 0.000031...... Validation Loss: 0.000030\n",
      "Epoch: 6/8..... Step: 12600..... Train Loss: 0.000001...... Validation Loss: 0.000007\n",
      "Epoch: 6/8..... Step: 12800..... Train Loss: 0.000007...... Validation Loss: 0.000023\n",
      "Epoch: 6/8..... Step: 13000..... Train Loss: 0.000002...... Validation Loss: 0.000022\n",
      "Epoch: 6/8..... Step: 13200..... Train Loss: 0.000000...... Validation Loss: 0.000021\n",
      "Epoch: 6/8..... Step: 13400..... Train Loss: 0.000114...... Validation Loss: 0.000361\n",
      "Epoch: 6/8..... Step: 13600..... Train Loss: 0.000036...... Validation Loss: 0.000038\n",
      "Epoch: 6/8..... Step: 13800..... Train Loss: 0.000011...... Validation Loss: 0.000022\n",
      "Epoch: 6/8..... Step: 14000..... Train Loss: 0.000020...... Validation Loss: 0.000033\n",
      "Epoch: 6/8..... Step: 14200..... Train Loss: 0.000037...... Validation Loss: 0.000087\n",
      "Epoch: 6/8..... Step: 14400..... Train Loss: 0.000018...... Validation Loss: 0.000131\n",
      "Epoch: 6/8..... Step: 14600..... Train Loss: 0.000005...... Validation Loss: 0.000091\n",
      "Epoch: 6/8..... Step: 14800..... Train Loss: 0.000006...... Validation Loss: 0.000023\n",
      "Epoch: 6/8..... Step: 15000..... Train Loss: 0.000007...... Validation Loss: 0.000008\n",
      "Epoch: 7/8..... Step: 15200..... Train Loss: 0.000013...... Validation Loss: 0.000019\n",
      "Epoch: 7/8..... Step: 15400..... Train Loss: 0.000001...... Validation Loss: 0.000008\n",
      "Epoch: 7/8..... Step: 15600..... Train Loss: 0.000006...... Validation Loss: 0.000013\n",
      "Epoch: 7/8..... Step: 15800..... Train Loss: 0.000003...... Validation Loss: 0.000015\n",
      "Epoch: 7/8..... Step: 16000..... Train Loss: 0.000006...... Validation Loss: 0.000005\n",
      "Epoch: 7/8..... Step: 16200..... Train Loss: 0.000001...... Validation Loss: 5.910504\n",
      "Epoch: 7/8..... Step: 16400..... Train Loss: 0.000006...... Validation Loss: 4.500092\n",
      "Epoch: 7/8..... Step: 16600..... Train Loss: 0.000038...... Validation Loss: 0.000044\n",
      "Epoch: 7/8..... Step: 16800..... Train Loss: 0.000003...... Validation Loss: 0.000030\n",
      "Epoch: 7/8..... Step: 17000..... Train Loss: 0.000013...... Validation Loss: 0.000014\n",
      "Epoch: 7/8..... Step: 17200..... Train Loss: 0.000003...... Validation Loss: 0.000008\n",
      "Epoch: 7/8..... Step: 17400..... Train Loss: 0.000001...... Validation Loss: 0.000007\n",
      "Epoch: 8/8..... Step: 17600..... Train Loss: 0.000141...... Validation Loss: 0.022074\n",
      "Epoch: 8/8..... Step: 17800..... Train Loss: 0.000001...... Validation Loss: 0.000073\n",
      "Epoch: 8/8..... Step: 18000..... Train Loss: 0.000010...... Validation Loss: 0.000074\n",
      "Epoch: 8/8..... Step: 18200..... Train Loss: 0.000001...... Validation Loss: 0.000004\n",
      "Epoch: 8/8..... Step: 18400..... Train Loss: 7.042450...... Validation Loss: 0.000880\n",
      "Epoch: 8/8..... Step: 18600..... Train Loss: 0.000018...... Validation Loss: 0.000080\n",
      "Epoch: 8/8..... Step: 18800..... Train Loss: 0.000041...... Validation Loss: 0.000025\n",
      "Epoch: 8/8..... Step: 19000..... Train Loss: 0.000001...... Validation Loss: 0.000020\n",
      "Epoch: 8/8..... Step: 19200..... Train Loss: 0.000000...... Validation Loss: 0.000019\n",
      "Epoch: 8/8..... Step: 19400..... Train Loss: 0.000001...... Validation Loss: 0.000019\n",
      "Epoch: 8/8..... Step: 19600..... Train Loss: 0.603219...... Validation Loss: 0.000005\n",
      "Epoch: 8/8..... Step: 19800..... Train Loss: 0.000007...... Validation Loss: 0.000032\n"
     ]
    }
   ],
   "source": [
    "# Train the Neural Network\n",
    "# Off-load the model to CUDA\n",
    "if gpu_available:\n",
    "    net.cuda()\n",
    "\n",
    "net.train()\n",
    "for e in range(epochs):\n",
    "    h = net.init_hidden(batch_size)\n",
    "    \n",
    "    for inputs, labels in train_loader:\n",
    "        count += 1\n",
    "        \n",
    "        if gpu_available:\n",
    "            inputs, labels = inputs.cuda(), labels.cuda()\n",
    "        h = tuple([each.data for each in h])\n",
    "        \n",
    "        net.zero_grad()\n",
    "        outputs, h = net(inputs, h)\n",
    "        loss = criterion(outputs.float(), labels.float())\n",
    "        \n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm(net.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        \n",
    "        if count % print_every == 0:\n",
    "            val_h = net.init_hidden(batch_size)\n",
    "            val_losses = []\n",
    "            net.eval()\n",
    "            \n",
    "            for inputs, labels in valid_loader:\n",
    "                val_h = tuple([each.data for each in val_h])\n",
    "                \n",
    "                if gpu_available:\n",
    "                    inputs, labels = inputs.cuda(), labels.cuda()\n",
    "                    \n",
    "            outputs, val_h = net(inputs, val_h)\n",
    "            val_loss = criterion(outputs.float(), labels.float())\n",
    "            val_losses.append(val_loss.item())\n",
    "        \n",
    "            net.train()\n",
    "            print(f\"Epoch: {e+1}/{epochs}.....\",f\"Step: {count}.....\",\"Train Loss: {:.6f}......\".format(loss.item()),\"Validation Loss: {:.6f}\".format(np.mean(val_losses)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Test Loss: 0.9368\n",
      "Average Test Accuracy: 0.8641\n"
     ]
    }
   ],
   "source": [
    "# Train the Neural Network\n",
    "test_losses = []\n",
    "num_correct = 0\n",
    "\n",
    "h = net.init_hidden(batch_size)\n",
    "net.eval()\n",
    "\n",
    "for inputs, labels in test_loader:\n",
    "    h = tuple([each.data for each in h])\n",
    "    \n",
    "    if gpu_available:\n",
    "        inputs, labels = inputs.cuda(), labels.cuda()\n",
    "    \n",
    "    outputs, h = net(inputs, h)\n",
    "    test_loss = criterion(outputs.float(), labels.float())\n",
    "    test_losses.append(test_loss.item())\n",
    "    \n",
    "    pred = torch.round(outputs.squeeze())\n",
    "    correct_tensor = pred.eq(labels.float().view_as(pred))\n",
    "    correct = np.squeeze(correct_tensor.numpy()) if not gpu_available else np.squeeze(correct_tensor.cpu().numpy())\n",
    "    num_correct += np.sum(correct)\n",
    "\n",
    "\n",
    "test_acc = num_correct/len(test_loader.dataset)\n",
    "\n",
    "print(\"Average Test Loss: {:.4f}\".format(np.mean(test_losses)))\n",
    "print(\"Average Test Accuracy: {:.4f}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Sentiment Analysis of COVID-19 Tweets with Deep LSTM\n",
      "\n",
      "------Available Options------\n",
      "1. Inference on Sample Tweets\n",
      "2. Enter Custom Tweets/Sentences\n",
      "3. Exit\n",
      "\n",
      "Please select an option from the above:\n",
      "\n",
      "--------------------------------------------------------------------------------------\n",
      "\n",
      " Original input sentence: Many lost their jobs because of covid and it is highly dangerous\n",
      "\n",
      " Pre-processed input sentence: ['many', 'lost', 'jobs', 'covid', 'highly', 'dangerous']\n",
      "\n",
      " Sentiment: Negative\n",
      "\n",
      "--------------------------------------------------------------------------------------\n",
      "\n",
      " Original input sentence: I am happy that my family members are safe in this tough times\n",
      "\n",
      " Pre-processed input sentence: ['happy', 'family', 'members', 'safe', 'tough', 'times']\n",
      "\n",
      " Sentiment: Positive\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nwhile(True):\\n    choice = int(input())\\n\\n    if choice == 1:\\n        sample_1 = \\'Many lost their jobs because of covid and it is highly dangerous\\'\\n        sentiment(net, sample_1)\\n    \\n        sample_2 = \\'I am happy that my family members are safe in this tough times\\'\\n        sentiment(net, sample_2)\\n    \\n    elif choice == 2:\\n        print(\"\\nPlease enter a sentence/tweet:\")\\n        user_input = input()\\n        sentiment(net, user_input)\\n    \\n    elif choice == 3:\\n        print(\"\\nExiting...\")\\n        break\\n        \\n'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Driver program\n",
    "print(\"Neural Sentiment Analysis of COVID-19 Tweets with Deep LSTM\")\n",
    "print(\"\\n------Available Options------\")\n",
    "print(\"1. Inference on Sample Tweets\")\n",
    "print(\"2. Enter Custom Tweets/Sentences\")\n",
    "print(\"3. Exit\")\n",
    "print(\"\\nPlease select an option from the above:\")\n",
    "\n",
    "sample_1 = 'Many lost their jobs because of covid and it is highly dangerous'\n",
    "sentiment(net, sample_1)\n",
    "    \n",
    "sample_2 = 'I am happy that my family members are safe in this tough times'\n",
    "sentiment(net, sample_2)\n",
    "\n",
    "\"\"\"\n",
    "while(True):\n",
    "    choice = int(input())\n",
    "\n",
    "    if choice == 1:\n",
    "        sample_1 = 'Many lost their jobs because of covid and it is highly dangerous'\n",
    "        sentiment(net, sample_1)\n",
    "    \n",
    "        sample_2 = 'I am happy that my family members are safe in this tough times'\n",
    "        sentiment(net, sample_2)\n",
    "    \n",
    "    elif choice == 2:\n",
    "        print(\"\\nPlease enter a sentence/tweet:\")\n",
    "        user_input = input()\n",
    "        sentiment(net, user_input)\n",
    "    \n",
    "    elif choice == 3:\n",
    "        print(\"\\nExiting...\")\n",
    "        break\n",
    "        \n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
